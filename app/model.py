from transformers import AutoProcessor, AutoModelForCausalLM, AutoTokenizer
import torch
import logging

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def load_model():
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ç–æ–ª—å–∫–æ –º–æ–¥–µ–ª—å Qwen2.5-0.5B-Instruct, –±–µ–∑ fallback
    """
    model_name = "Qwen/Qwen2.5-0.5B-Instruct"
    
    try:
        logger.info(f"üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏: {model_name}")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        logger.info("‚öôÔ∏è –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞...")
        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
        logger.info("üß† –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏...")
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype="auto",
            trust_remote_code=True
        )
        
        logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å {model_name} —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        logger.info(f"üìä –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏: {model.device}")
        logger.info(f"üéØ –¢–∏–ø –¥–∞–Ω–Ω—ã—Ö: {model.dtype}")
        
        return model, tokenizer
        
    except Exception as e:
        logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ {model_name}: {str(e)}")
        logger.error("üö® –°–∏—Å—Ç–µ–º–∞ –Ω–µ –º–æ–∂–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –±–µ–∑ –æ—Å–Ω–æ–≤–Ω–æ–π –º–æ–¥–µ–ª–∏")
        raise Exception(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å {model_name}: {str(e)}")

def test_model_loading():
    """
    –¢–µ—Å—Ç–æ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏
    """
    try:
        logger.info("üß™ –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏...")
        model, tokenizer = load_model()
        
        # –ü—Ä–æ—Å—Ç–æ–π —Ç–µ—Å—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        logger.info("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏...")
        test_input = "–ü—Ä–∏–≤–µ—Ç! –ö–∞–∫ –¥–µ–ª–∞?"
        inputs = tokenizer(test_input, return_tensors="pt", padding=True)
        
        with torch.no_grad():
            outputs = model.generate(**inputs, max_new_tokens=20, do_sample=False, pad_token_id=tokenizer.eos_token_id)
        
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        logger.info(f"üß™ –¢–µ—Å—Ç–æ–≤—ã–π –æ—Ç–≤–µ—Ç: {response}")
        logger.info("‚úÖ –¢–µ—Å—Ç –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ –ø—Ä–æ—à–µ–ª —É—Å–ø–µ—à–Ω–æ")
        return True
    except Exception as e:
        logger.error(f"‚ùå –¢–µ—Å—Ç –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ –ø—Ä–æ–≤–∞–ª–µ–Ω: {str(e)}")
        return False