from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from app.model import load_model, test_model_loading
from fastapi.responses import HTMLResponse, JSONResponse
from typing import Optional
import traceback
import torch
import logging
import sys

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

app = FastAPI(title="LLM API", description="API –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å Qwen2.5-0.5B")

# –õ–µ–Ω–∏–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
model = None
tokenizer = None
model_info = {}

def get_model():
    global model, tokenizer, model_info
    if model is None or tokenizer is None:
        try:
            logger.info("üîÑ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏...")
            model, tokenizer = load_model()
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª–∏
            model_info = {
                "model_class": str(type(model).__name__),
                "tokenizer_class": str(type(tokenizer).__name__),
                "device": str(model.device) if hasattr(model, 'device') else "unknown",
                "dtype": str(model.dtype) if hasattr(model, 'dtype') else "unknown",
                "model_name": getattr(model, "name_or_path", "unknown"),
                "vocab_size": tokenizer.vocab_size if hasattr(tokenizer, 'vocab_size') else "unknown"
            }
            logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {model_info}")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {str(e)}")
            logger.error(f"üìã –ü–æ–ª–Ω—ã–π —Ç—Ä–µ–π—Å–±–µ–∫: {traceback.format_exc()}")
            raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {str(e)}")
    return model, tokenizer

class Prompt(BaseModel):
    text: str
    system_prompt: Optional[str] = "–í—ã - –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç, —Å–ø–æ—Å–æ–±–Ω—ã–π –æ—Ç–≤–µ—á–∞—Ç—å –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã."

@app.get("/", response_class=HTMLResponse)
def root():
    return """
    <html>
        <head>
            <title>LLM API - –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞</title>
            <meta charset="utf-8">
            <style>
                body { font-family: Arial, sans-serif; margin: 40px; }
                .status { padding: 10px; margin: 10px 0; border-radius: 5px; }
                .success { background-color: #d4edda; color: #155724; }
                .error { background-color: #f8d7da; color: #721c24; }
                .info { background-color: #d1ecf1; color: #0c5460; }
            </style>
        </head>
        <body>
            <h1> LLM API - Qwen2.5-0.5B</h1>
            <div class="info status">
                <h3>üìä –î–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∏–µ —ç–Ω–¥–ø–æ–∏–Ω—Ç—ã:</h3>
                <ul>
                    <li><a href="/health">/health</a> - –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è API</li>
                    <li><a href="/model-info">/model-info</a> - –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏</li>
                    <li><a href="/test-model">/test-model</a> - –¢–µ—Å—Ç –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏</li>
                    <li><a href="/docs">/docs</a> - OpenAPI –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è</li>
                </ul>
            </div>
            
            <div class="info status">
                <h3>üöÄ API —ç–Ω–¥–ø–æ–∏–Ω—Ç—ã:</h3>
                <ul>
                    <li><code>POST /generate</code> - –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞</li>
                    <li><code>POST /simple-chat</code> - –ü—Ä–æ—Å—Ç–æ–π —Ç–µ—Å—Ç–æ–≤—ã–π —á–∞—Ç</li>
                </ul>
            </div>
            
            <p>üåê UI –¥–æ—Å—Ç—É–ø–µ–Ω –ø–æ –∞–¥—Ä–µ—Å—É <a href="http://localhost:7860" target="_blank">http://localhost:7860</a></p>
        </body>
    </html>
    """

@app.get("/health")
def health():
    try:
        model, tokenizer = get_model()
        return {
            "status": "healthy", 
            "model_loaded": True,
            "model_info": model_info,
            "torch_version": torch.__version__,
            "cuda_available": torch.cuda.is_available(),
            "device_count": torch.cuda.device_count() if torch.cuda.is_available() else 0
        }
    except Exception as e:
        logger.error(f"‚ùå Health check failed: {str(e)}")
        return {
            "status": "unhealthy", 
            "model_loaded": False, 
            "error": str(e),
            "torch_version": torch.__version__,
            "cuda_available": torch.cuda.is_available()
        }

@app.get("/model-info")
def model_info_endpoint():
    """–ü–æ–¥—Ä–æ–±–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏"""
    try:
        model, tokenizer = get_model()
        return {
            "model_info": model_info,
            "transformers_version": __import__('transformers').__version__,
            "torch_version": torch.__version__,
            "device_info": {
                "cuda_available": torch.cuda.is_available(),
                "device_count": torch.cuda.device_count() if torch.cuda.is_available() else 0,
                "current_device": torch.cuda.current_device() if torch.cuda.is_available() else None
            }
        }
    except Exception as e:
        return {"error": str(e), "traceback": traceback.format_exc()}

@app.get("/test-model")
def test_model_endpoint():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏"""
    try:
        success = test_model_loading()
        return {"test_passed": success, "message": "–¢–µ—Å—Ç –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ –∑–∞–≤–µ—Ä—à–µ–Ω"}
    except Exception as e:
        return {"test_passed": False, "error": str(e), "traceback": traceback.format_exc()}

@app.post("/generate")
def generate(prompt: Prompt):
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞"""
    try:
        logger.info(f"üìù –ü–æ–ª—É—á–µ–Ω –∑–∞–ø—Ä–æ—Å –Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é: {prompt.text[:50]}...")
        model, tokenizer = get_model()
        
        # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç–æ–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        input_text = f"System: {prompt.system_prompt}\nUser: {prompt.text}\nAssistant:"
        logger.info(f"üî§ –ü—Ä–æ–º–ø—Ç —Å–æ–∑–¥–∞–Ω, –¥–ª–∏–Ω–∞: {len(input_text)}")
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Ö–æ–¥—ã
        logger.info("‚öôÔ∏è –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
        if hasattr(tokenizer, 'tokenizer'):
            # –≠—Ç–æ AutoTokenizer
            inputs = tokenizer(input_text, return_tensors="pt", padding=True, truncation=True, max_length=2048)
        else:
            # –≠—Ç–æ AutoTokenizer
            inputs = tokenizer(input_text, return_tensors="pt", padding=True, truncation=True, max_length=2048)
        
        inputs = inputs.to(model.device)
        logger.info(f"üìä –í—Ö–æ–¥—ã –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã, —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {model.device}")
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
        logger.info("üß† –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞...")
        with torch.no_grad():
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º pad_token_id –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id
            eos_token_id = tokenizer.eos_token_id
            
            output_ids = model.generate(
                **inputs, 
                max_new_tokens=512, 
                do_sample=True, 
                temperature=0.7,
                pad_token_id=pad_token_id,
                eos_token_id=eos_token_id,
                no_repeat_ngram_size=3
            )
        
        # –î–µ–∫–æ–¥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        logger.info("üî§ –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞...")
        full_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–π —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        if "Assistant:" in full_output:
            generated_text = full_output.split("Assistant:")[-1].strip()
        else:
            generated_text = full_output[len(input_text):].strip()
        
        logger.info(f"‚úÖ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞, –¥–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞: {len(generated_text)}")
        return {"response": generated_text, "input_length": len(input_text), "output_length": len(generated_text)}
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {str(e)}")
        logger.error(f"üìã –ü–æ–ª–Ω—ã–π —Ç—Ä–µ–π—Å–±–µ–∫: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {str(e)}")

@app.post("/simple-chat")
def simple_chat(prompt: Prompt):
    """–£–ø—Ä–æ—â–µ–Ω–Ω—ã–π —á–∞—Ç –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    try:
        logger.info(f"üí¨ –ü—Ä–æ—Å—Ç–æ–π —á–∞—Ç: {prompt.text}")
        return {
            "response": f"–ü—Ä–æ—Å—Ç–æ–π —Ç–µ—Å—Ç–æ–≤—ã–π –æ—Ç–≤–µ—Ç –Ω–∞: '{prompt.text}'. API —Ä–∞–±–æ—Ç–∞–µ—Ç! üéâ", 
            "status": "test_mode",
            "timestamp": __import__('datetime').datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ –ø—Ä–æ—Å—Ç–æ–º —á–∞—Ç–µ: {str(e)}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞: {str(e)}")

# –î–æ–±–∞–≤–ª—è–µ–º startup event –¥–ª—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏
@app.on_event("startup")
async def startup_event():
    logger.info("üöÄ –ó–∞–ø—É—Å–∫ API...")
    logger.info("‚ÑπÔ∏è –ú–æ–¥–µ–ª—å –±—É–¥–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—Ä–æ—Å–µ")

@app.on_event("shutdown")
async def shutdown_event():
    logger.info("üëã –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã API...")